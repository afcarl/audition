import pandas as pd
import numpy as np
import copy
import logging
from audition.plotting import plot_cats


def highest_metric_value(metric, metric_param, df):
    """Pick the model group with the highest metric value

    Arguments:
        metric (string) -- model evaluation metric, such as 'precision@'
        metric_param (string) -- model evaluation metric parameter,
            such as '300_abs'
        df (pandas.DataFrame) -- dataframe that is keyed on model group id,
            containing the columns:
                model_id,
                train_end_time,
                metric,
                parameter,
                raw_value,
                below_best,
                below_best_next_time
    Returns: (int) the model group id to select, with highest raw metric value
    """
    return df[df['raw_value'] == df['raw_value'].max()].index.tolist()[0]


SELECTION_RULES = {
    'highest_metric_value': highest_metric_value,
}


def plot_best_dist(metric, metric_param, df_best_dist, **plt_format_args):
    """Generates a plot of the percentage of time that a model group is within X percentage points
       of the best-performing model group using a given metric. At each point in time that a set of
       model groups is evaluated, the performance of the best model is calculated and the difference
       in performace for all other models found relative to this. An (x,y) point for a given model 
       group on the plot generated by this method means that across all of those tets sets, the model
       from that model group performed within X percentage points of the best model in y% of the test
       sets. 
       The plot will contain a line for each model group in the ExperimentEvaluation object
       representing the cumulative percent of the time that the group is within Xpp of the best group
       for each value of X between 0 and 100. All groups ultimately reach (1,1) on this graph (as every
       model group must be within 100pp of the best model 100% of the time), and a model specification
       that always dominated the others in the experiment would start at (0,1) and remain at y=1
       across the graph.

    Arguments:
        metric (string) -- model evaluation metric, such as 'precision@'
        metric_param (string) -- model evaluation metric parameter, such as '300_abs'
        df_best_dist (pandas.DataFrame)
        **plt_format_args -- formatting arguments passed through to plot_cats()
    """

    cat_col = 'model_type'
    plt_title = 'Fraction of models X pp worse than best {} {}'.format(metric, metric_param)

    plot_cats(
        df_best_dist,
        'pct_diff',
        'pct_of_time',
        cat_col=cat_col,
        title=plt_title,
        x_label='decrease in {} from best model'.format(metric),
        y_label='fraction of models',
        x_ticks=np.arange(0,1.1,0.1),
        **plt_format_args
    )



class ModelSelector(object):
    def __init__(self, db_engine, models_table, distance_table):
        self.db_engine = db_engine
        self.models_table = models_table
        self.distance_table = distance_table

    def _create_distance_table(self):
        self.db_engine.execute('''create table {} (
            model_group_id int,
            model_id int,
            train_end_time timestamp,
            metric text,
            parameter text,
            raw_value float,
            below_best float,
            below_best_next_time float
        )'''.format(self.distance_table))

    def _populate_distance_table(self, model_group_ids, metrics):
        # TODO: allow min cutoff train date
        # TODO: only include immediate eval
        # TODO: yell at user if not all model groups included have the same test sets
        for metric in metrics:
            self.db_engine.execute('''
                insert into {new_table}
                WITH model_ranks AS (
                    SELECT
                        m.model_group_id,
                        m.model_id,
                        m.train_end_time,
                        ev.value,
                        row_number() OVER (
                            PARTITION BY m.train_end_time
                            ORDER BY ev.value DESC, RANDOM()
                        ) AS rank
                  FROM results.evaluations ev
                  JOIN results.{models_table} m USING(model_id)
                  JOIN results.model_groups mg USING(model_group_id)
                  WHERE ev.metric='{metric}' AND ev.parameter='{metric_param}'
                        AND m.model_group_id IN ({model_group_ids})
                ),
                model_tols AS (
                  SELECT train_end_time, model_group_id, model_id,
                         rank,
                         value,
                         first_value(value) over (
                            partition by train_end_time
                            order by rank ASC
                        ) AS best_val
                  FROM model_ranks
                ),
                current_best_vals as (
                    SELECT
                        model_group_id,
                        model_id,
                        train_end_time,
                        '{metric}',
                        '{metric_param}',
                        value as raw_value,
                        best_val - value below_best
                    FROM model_tols
                )
                select
                    current_best_vals.*,
                    first_value(below_best) over (
                        partition by model_group_id
                        order by train_end_time asc
                        rows between 1 following and unbounded following
                    ) below_best_next_time
                from current_best_vals
            '''.format(
                model_group_ids=self._str_in_sql(model_group_ids),
                models_table=self.models_table,
                metric=metric['metric'],
                metric_param=metric['param'],
                new_table=self.distance_table
            ))

    def create_and_populate_distance_table(
        self,
        model_group_ids,
        metrics,
    ):
        self._create_distance_table()
        self._populate_distance_table(model_group_ids, metrics)

    def _str_in_sql(self, values):
        return ','.join(map(lambda x: "'{}'".format(x), values))

    def get_best_dist(
        self,
        metric,
        metric_param,
        model_group_ids,
        train_end_times,
        max_below_best,
    ):
        """Fetch a best distance data frame from the distance table

        Arguments:
            metric (string) -- model evaluation metric, such as 'precision@'
            metric_param (string) -- model evaluation metric parameter,
                such as '300_abs'
        """


        model_group_union_sql = ' union all '.join([
            '(select {} as model_group_id)'.format(model_group_id)
            for model_group_id in model_group_ids
        ])
        sel_params = {
            'metric': metric,
            'metric_param': metric_param,
            'model_group_union_sql': model_group_union_sql,
            'distance_table': self.distance_table,
            'max_below_best': max_below_best,
            'model_group_str': self._str_in_sql(model_group_ids),
            'train_end_str': self._str_in_sql(train_end_times),
        }
        sel = """
                with model_group_ids as ({model_group_union_sql}),
                x_vals AS (
                  SELECT m.model_group_id, s.pct_diff
                  FROM
                  (
                  SELECT GENERATE_SERIES(0,100) / 100.0 AS pct_diff
                  ) s
                  CROSS JOIN
                  (
                  SELECT DISTINCT model_group_id FROM model_group_ids
                  ) m
                )
                SELECT dist.model_group_id, pct_diff, mg.model_type,
                       COUNT(*) AS num_models,
                       AVG(CASE WHEN below_best <= pct_diff THEN 1 ELSE 0 END) AS pct_of_time
                FROM {distance_table} dist
                JOIN x_vals USING(model_group_id)
                JOIN results.model_groups mg using (model_group_id)
                WHERE
                    dist.metric='{metric}'
                    AND dist.parameter='{metric_param}'
                    and pct_diff <= {max_below_best}
                    and below_best <= {max_below_best}
                    and model_group_id in ({model_group_str})
                    and train_end_time in ({train_end_str})
                GROUP BY 1,2,3
            """.format(**sel_params)

        return pd.read_sql(sel, self.db_engine)

    def model_groups_past_threshold_as_of(
        self,
        metric_filters,
        model_group_ids,
        train_end_time
    ):
        query = ' intersect '.join(['''
            select model_group_id
            from {dist_table} dist
            where
            train_end_time = '{train_end_time}'
            and model_group_id in ({model_group_ids})
            and metric = '{metric}'
            and parameter = '{parameter}'
            and below_best < {max_below}
            and raw_value >= {min_value}
        '''.format(
            dist_table=self.distance_table,
            model_group_ids=self._str_in_sql(model_group_ids),
            train_end_time=train_end_time,
            metric=metric_filter['metric'],
            parameter=metric_filter['metric_param'],
            max_below=metric_filter['max_below_best'],
            min_value=metric_filter['min_value']
        ) for metric_filter in metric_filters])
        return set(row[0] for row in self.db_engine.execute(query))

    def model_groups_past_threshold(
        self,
        metric_filters,
        model_group_ids,
        train_end_times
    ):
        total_model_groups = set()
        for train_end_time in train_end_times:
            passing_model_group_ids = self.model_groups_past_threshold_as_of(
                metric_filters=metric_filters,
                model_group_ids=model_group_ids,
                train_end_time=train_end_time
            )
            logging.info(
                'Found %s model groups past threshold for %s',
                len(passing_model_group_ids),
                train_end_time
            )
            total_model_groups |= passing_model_group_ids
        logging.info(
            'Found %s total model groups past threshold',
            len(total_model_groups)
        )
        return total_model_groups
                         
    def calculate_regrets(
        self,
        selection_rule,
        model_group_ids,
        train_end_times,
        metric,
        metric_param
    ):
        """Calculate the regrets, or distance between the chosen model and
            the maximum value next test time

        Arguments:
            selection_rule
            model_group_ids
            train_end_times
            metric
            metric_param

        Returns: (list) for each train end time, the distance between the
            model group chosen by the selection rule and the potential
            maximum for the next train end time
        """
        regrets = []
        for train_end_time in train_end_times:
            df = pd.read_sql(
                '''select * from {distance_table}
                where train_end_time = '{train_end_time}'
                and metric = '{metric}'
                and parameter = '{metric_param}'
                and model_group_id in ({model_group_str})
                '''.format(
                    distance_table=self.distance_table,
                    train_end_time=train_end_time,
                    metric=metric,
                    metric_param=metric_param,
                    model_group_str=self._str_in_sql(model_group_ids),
                ),
                self.db_engine
            ).set_index('model_group_id')
            choice = selection_rule(metric, metric_param, df)
            regret_result = self.db_engine.execute('''
                select below_best_next_time
                from {distance_table}
                where model_group_id = %(model_group_id)s
                and train_end_time = %(train_end_time)s
                and metric = %(metric)s
                and parameter = %(metric_param)s
            '''.format(distance_table=self.distance_table),
                model_group_id=int(choice),
                train_end_time=train_end_time,
                metric=metric,
                metric_param=metric_param
            )
            regrets.append([row[0] for row in regret_result][0])
        return regrets

    def plot_all_best_dist(self, metric_filters, model_group_ids, train_end_times):
        for metric_filter in metric_filters:
            df = self.get_best_dist(
                metric=metric_filter['metric'],
                metric_param=metric_filter['metric_param'],
                max_below_best=metric_filter['max_below_best'],
                model_group_ids=model_group_ids,
                train_end_times=train_end_times
            )
            plot_best_dist(
                metric=metric_filter['metric'],
                metric_param=metric_filter['metric_param'],
                df_best_dist=df
            )

